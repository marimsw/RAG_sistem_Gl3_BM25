{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPy61mtXjGeT6NRUdWTPF7M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marimsw/RAG_sistem_Gl3_BM25/blob/main/Gl_3_1_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF: фундаментальная модель     \n",
        "Подход Term Frequency – Inverse Document Frequency остаётся краеугольным\n",
        "камнем информационного поиска. Метод основан на простой идее: важность\n",
        "термина в документе пропорциональна частоте его встречаемости в этом\n",
        "документе и обратно пропорциональна частоте встречаемости во всей коллекции."
      ],
      "metadata": {
        "id": "d7jiKs_jxRTO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8JKlGbmCBnUc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Set\n",
        "class TFIDFRetriever:\n",
        "  def __init__(self):\n",
        "      \"\"\"Инициализация TF-IDF ретривера\"\"\"\n",
        "      self.vocabulary: Set[str] = set()\n",
        "      self.documents: List[List[str]] = []\n",
        "      self.document_frequencies: Dict[str, int] = defaultdict(int)\n",
        "      self.tfidf_matrix: np.ndarray = None\n",
        "      self.idf_values: Dict[str, float] = {}\n",
        "  def _tokenize(self, text: str) -> List[str]:\n",
        "      \"\"\"Простая токенизация текста\"\"\"\n",
        "      # В реальных системах используйте более продвинутые методы\n",
        "      return text.lower().split()\n",
        "  def _calculate_tf(self, term_count: int, total_terms: int) -> float:\n",
        "      \"\"\"Вычисление Term Frequency\"\"\"\n",
        "      if total_terms == 0:\n",
        "        return 0.0\n",
        "      return term_count / total_terms\n",
        "  def _calculate_idf(self, term: str) -> float:\n",
        "      \"\"\"Вычисление Inverse Document Frequency\"\"\"\n",
        "      if term not in self.document_frequencies:\n",
        "        return 0.0\n",
        "      # Добавляем 1 к знаменателю для избежания деления на ноль\n",
        "      idf = math.log(len(self.documents) / (self.document_frequencies[term] + 1))\n",
        "      return idf\n",
        "  def fit(self, documents: List[str]):\n",
        "      \"\"\"Обучение модели на коллекции документов\"\"\"\n",
        "      print(\"Токенизация документов...\")\n",
        "      self.documents = [self._tokenize(doc) for doc in documents]\n",
        "      # Построение словаря и подсчёт частот документов\n",
        "      print(\"Построение словаря...\")\n",
        "      for doc_tokens in self.documents:\n",
        "          unique_terms = set(doc_tokens)\n",
        "          self.vocabulary.update(unique_terms)\n",
        "          for term in unique_terms:\n",
        "              self.document_frequencies[term] += 1\n",
        "              print(f\"Размер словаря: {len(self.vocabulary)}\")\n",
        "            # Вычисление IDF для всех терминов\n",
        "          print(\"Вычисление IDF...\")\n",
        "          for term in self.vocabulary:\n",
        "            self.idf_values[term] = self._calculate_idf(term)\n",
        "          # Построение TF-IDF матрицы\n",
        "          print(\"Построение TF-IDF матрицы...\")\n",
        "          self._build_tfidf_matrix()\n",
        "  def _build_tfidf_matrix(self):\n",
        "      \"\"\"Построение матрицы TF-IDF для всех документов\"\"\"\n",
        "      vocab_list = list(self.vocabulary)\n",
        "      self.vocab_to_index = {term: i for i, term in enumerate(vocab_list)}\n",
        "      # Инициализация матрицы\n",
        "      num_docs = len(self.documents)\n",
        "      vocab_size = len(self.vocabulary)\n",
        "      self.tfidf_matrix = np.zeros((num_docs, vocab_size))\n",
        "      for doc_idx, doc_tokens in enumerate(self.documents):\n",
        "       # Подсчёт частот терминов в документе\n",
        "        term_counts = Counter(doc_tokens)\n",
        "        total_terms = len(doc_tokens)\n",
        "        for term, count in term_counts.items():\n",
        "         if term in self.vocab_to_index:\n",
        "            term_idx = self.vocab_to_index[term]\n",
        "            tf = self._calculate_tf(count, total_terms)\n",
        "            idf = self.idf_values[term]\n",
        "            self.tfidf_matrix[doc_idx, term_idx] = tf * idf\n",
        "  def _vectorize_query(self, query: str) -> np.ndarray:\n",
        "      \"\"\"Преобразование запроса в TF-IDF вектор\"\"\"\n",
        "      query_tokens = self._tokenize(query)\n",
        "      query_vector = np.zeros(len(self.vocabulary))\n",
        "      if not query_tokens:\n",
        "        return query_vector\n",
        "      term_counts = Counter(query_tokens)\n",
        "      total_terms = len(query_tokens)\n",
        "      for term, count in term_counts.items():\n",
        "        if term in self.vocab_to_index:\n",
        "          term_idx = self.vocab_to_index[term]\n",
        "          tf = self._calculate_tf(count, total_terms)\n",
        "          idf = self.idf_values.get(term, 0.0)\n",
        "          query_vector[term_idx] = tf * idf\n",
        "      return query_vector\n",
        "  def search(self, query: str, top_k: int = 5) -> List[tuple]:\n",
        "    \"\"\"Поиск наиболее релевантных документов\"\"\"\n",
        "    if self.tfidf_matrix is None:\n",
        "      raise ValueError(\"Модель не обучена. Вызовите fit() перед поиском.\")\n",
        "\n",
        "\n",
        "    # Векторизация запроса\n",
        "    query_vector = self._vectorize_query(query)\n",
        "    # Вычисление косинусного сходства\n",
        "    # Нормализация векторов\n",
        "    query_norm = np.linalg.norm(query_vector)\n",
        "    doc_norms = np.linalg.norm(self.tfidf_matrix, axis=1)\n",
        "    if query_norm == 0:\n",
        "      return []\n",
        "    # Косинусное сходство\n",
        "    similarities = np.dot(self.tfidf_matrix, query_vector) / (doc_norms * query_norm +\n",
        "    1e-8)\n",
        "    # Получение топ-K результатов\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "      if similarities[idx] > 0: # Только положительные сходства\n",
        "        results.append((idx, similarities[idx]))\n",
        "    return results\n",
        "  def explain_scoring(self, query: str, doc_idx: int) -> Dict:\n",
        "    \"\"\"Детальное объяснение подсчёта релевантности\"\"\"\n",
        "    query_tokens = self._tokenize(query)\n",
        "    doc_tokens = self.documents[doc_idx]\n",
        "    explanation = {\n",
        "    'query_tokens': query_tokens,\n",
        "    'document_tokens': doc_tokens,\n",
        "    'term_scores': {}\n",
        "    }\n",
        "    doc_term_counts = Counter(doc_tokens)\n",
        "    query_term_counts= Counter(query_tokens)\n",
        "    total_doc_terms = len(doc_tokens)\n",
        "    total_query_terms = len(query_tokens)\n",
        "\n",
        "\n",
        "    for term in set(query_tokens):\n",
        "      if term in self.vocab_to_index:\n",
        "          # TF для документа и запроса\n",
        "          doc_tf = self._calculate_tf(doc_term_counts[term], total_doc_terms)\n",
        "          query_tf = self._calculate_tf(query_term_counts[term], total_query_terms)\n",
        "          # IDF\n",
        "          idf = self.idf_values[term]\n",
        "          # TF-IDF для документа и запроса\n",
        "          doc_tfidf = doc_tf * idf\n",
        "          query_tfidf = query_tf * idf\n",
        "          explanation['term_scores'][term] = {\n",
        "\n",
        "          'document_tf': doc_tf,\n",
        "          'query_tf': query_tf,\n",
        "          'idf': idf,\n",
        "          'document_tfidf': doc_tfidf,\n",
        "          'query_tfidf': query_tfidf,\n",
        "          'contribution': doc_tfidf * query_tfidf\n",
        "          }\n",
        "    return explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Продвинутая реализация ВМ25"
      ],
      "metadata": {
        "id": "VPcbIDqawTkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BM25: вероятностная эволюция     \n",
        "Алгоритм BM25 представляет собой вероятностную модель поиска, превосходящую TF-IDF за счёт учёта насыщения частоты терминов и нормализации\n",
        "длины документов. Алгоритм основан на модели вероятностного ранжирования Робертсона–Спарка–Джонса."
      ],
      "metadata": {
        "id": "OuqIbGgtw8Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Tuple\n",
        "class BM25Retriever:\n",
        "  def __init__(self, k1: float = 1.2, b: float = 0.75):\n",
        "      \"\"\"\n",
        "      Инициализация BM25-ретривера\n",
        "      Args:\n",
        "        k1: параметр насыщения частоты терминов (типично 1.2–2.0)\n",
        "        b: параметр нормализации длины документа (типично 0.75)\n",
        "      \"\"\"\n",
        "      self.k1 = k1\n",
        "      self.b = b\n",
        "      self.documents: List[List[str]] = []\n",
        "      self.document_lengths: List[int] = []\n",
        "      self.average_document_length: float = 0.0\n",
        "      self.vocabulary: Dict[str, int] = {}\n",
        "      self.document_frequencies: Dict[str, int] = defaultdict(int)\n",
        "      self.idf_cache: Dict[str, float] = {}\n",
        "      self.term_document_matrix: Dict[str, Dict[int, int]] = defaultdict(dict)\n",
        "  def _tokenize(self, text: str) -> List[str]:\n",
        "      \"\"\"Токенизация с основной предобработкой\"\"\"\n",
        "      # В продакшн-системах используйте более сложную предобработку\n",
        "      tokens = text.lower().split()\n",
        "      # Удаление пунктуации и коротких токенов\n",
        "      cleaned_tokens = [token.strip('.,!?\";:()[]{}') for token in tokens]\n",
        "      return [token for token in cleaned_tokens if len(token) > 1]\n",
        "  def _calculate_idf(self, term: str) -> float:\n",
        "      \"\"\"Вычисление IDF с BM25-сглаживанием\"\"\"\n",
        "      if term not in self.document_frequencies:\n",
        "        return 0.0\n",
        "      # BM25 IDF с добавлением 0.5 для сглаживания\n",
        "      N = len(self.documents)\n",
        "      df = self.document_frequencies[term]\n",
        "      # Избегаем отрицательных IDF\n",
        "      numerator = N - df + 0.5\n",
        "      denominator = df + 0.5\n",
        "      if numerator <= 0:\n",
        "        return 0.0\n",
        "      idf = math.log(numerator / denominator)\n",
        "      return idf\n",
        "  def fit(self, documents: List[str]):\n",
        "      \"\"\"Построение индекса BM25\"\"\"\n",
        "      print(\"Подготовка корпуса документов...\")\n",
        "      # Токенизация всех документов\n",
        "      self.documents = [self._tokenize(doc) for doc in documents]\n",
        "      self.document_lengths = [len(doc_tokens) for doc_tokens in self.documents]\n",
        "      if not self.document_lengths:\n",
        "        raise ValueError(\"Корпус документов пуст\")\n",
        "      self.average_document_length = sum(self.document_lengths) / len(self.document_lengths)\n",
        "      # Построение словаря и подсчёт статистик\n",
        "      print(\"Построение словаря и статистик...\")\n",
        "      term_id = 0\n",
        "      for doc_idx, doc_tokens in enumerate(self.documents):\n",
        "        term_counts = Counter(doc_tokens)\n",
        "        doc_unique_terms = set(doc_tokens)\n",
        "        # Обновление частот документов\n",
        "        for term in doc_unique_terms:\n",
        "          self.document_frequencies[term] += 1\n",
        "      # Присвоение ID новым терминам\n",
        "          if term not in self.vocabulary:\n",
        "            self.vocabulary[term] = term_id\n",
        "            term_id += 1\n",
        "      # Сохранение частот терминов для каждого документа\n",
        "      for term, count in term_counts.items():\n",
        "        self.term_document_matrix[term][doc_idx] = count\n",
        "      print(f\"Словарь содержит {len(self.vocabulary)} уникальных терминов\")\n",
        "      print(f\"Средняя длина документа: {self.average_document_length:.2f}\")\n",
        "      # Предвычисление IDF для всех терминов\n",
        "      print(\"Вычисление IDF...\")\n",
        "      for term in self.vocabulary:\n",
        "        self.idf_cache[term] = self._calculate_idf(term)\n",
        "  def _calculate_bm25_score(self, query_terms: List[str], doc_idx: int) -> float:\n",
        "      \"\"\"Вычисление BM25-скора для документа\"\"\"\n",
        "      if doc_idx >= len(self.documents):\n",
        "        return 0.0\n",
        "      doc_length = self.document_lengths[doc_idx]\n",
        "      score = 0.0\n",
        "      # Нормализация длины документа\n",
        "      length_normalization = 1 - self.b + self.b * (doc_length / self.average_document_length)\n",
        "      query_term_counts = Counter(query_terms)\n",
        "      for term, query_tf in query_term_counts.items():\n",
        "        if term not in self.vocabulary:\n",
        "          continue\n",
        "        # IDF-компонент\n",
        "        idf = self.idf_cache.get(term, 0.0)\n",
        "        # Частота термина в документе\n",
        "        doc_tf = self.term_document_matrix[term].get(doc_idx, 0)\n",
        "        # BM25-формула для термина\n",
        "        numerator = doc_tf * (self.k1 + 1)\n",
        "        denominator = doc_tf + self.k1 * length_normalization\n",
        "        tf_component = numerator / denominator if denominator > 0 else 0\n",
        "        # Учитываем частоту термина в запросе (обычно 1)\n",
        "\n",
        "        term_score = idf * tf_component * query_tf\n",
        "        score += term_score\n",
        "      return score\n",
        "  def search(self, query: str, top_k: int = 5) -> List[Tuple[int, float]]:\n",
        "      \"\"\"Поиск с BM25-скорингом\"\"\"\n",
        "      if not self.documents:\n",
        "        raise ValueError(\"Индекс не построен. Вызовите fit() перед поиском.\")\n",
        "      query_terms = self._tokenize(query)\n",
        "      if not query_terms:\n",
        "        return []\n",
        "      print(f\"Поиск по запросу: {query_terms}\")\n",
        "      # Вычисление скоров для всех документов\n",
        "      scores = []\n",
        "      for doc_idx in range(len(self.documents)):\n",
        "        score = self._calculate_bm25_score(query_terms, doc_idx)\n",
        "        if score > 0: # Только положительные скоры\n",
        "          scores.append((doc_idx, score))\n",
        "      # Сортировка по убыванию скора\n",
        "      scores.sort(key=lambda x: x[1], reverse=True)\n",
        "      return scores[:top_k]\n",
        "  def explain_scoring(self, query: str, doc_idx: int) -> Dict:\n",
        "      \"\"\"Подробное объяснение BM25-скоринга\"\"\"\n",
        "      query_terms = self._tokenize(query)\n",
        "      if doc_idx >= len(self.documents):\n",
        "        return {'error': 'Недопустимый индекс документа'}\n",
        "      doc_tokens = self.documents[doc_idx]\n",
        "      doc_length = self.document_lengths[doc_idx]\n",
        "      length_norm = 1 - self.b + self.b * (doc_length / self.average_document_length)\n",
        "      explanation = {\n",
        "      'query_terms': query_terms,\n",
        "      'document_length': doc_length,\n",
        "      'average_document_length': self.average_document_length,\n",
        "      'length_normalization': length_norm,\n",
        "      'parameters': {'k1': self.k1, 'b': self.b},\n",
        "      'term_contributions': {},\n",
        "      'total_score': 0.0\n",
        "      }\n",
        "      query_term_counts = Counter(query_terms)\n",
        "      total_score = 0.0\n",
        "      for term, query_tf in query_term_counts.items():\n",
        "        if term in self.vocabulary:\n",
        "          idf = self.idf_cache[term]\n",
        "          doc_tf = self.term_document_matrix[term].get(doc_idx, 0)\n",
        "          df = self.document_frequencies[term]\n",
        "          # Компоненты BM25\n",
        "          tf_numerator = doc_tf * (self.k1 + 1)\n",
        "          tf_denominator = doc_tf + self.k1 * length_norm\n",
        "          tf_component = tf_numerator / tf_denominator if tf_denominator > 0 else 0\n",
        "          term_score = idf * tf_component * query_tf\n",
        "          total_score += term_score\n",
        "          explanation['term_contributions'][term] = {\n",
        "          'document_frequency': df,\n",
        "          'document_tf': doc_tf,\n",
        "          'query_tf': query_tf,\n",
        "          'idf': idf,\n",
        "          'tf_component': tf_component,\n",
        "          'term_score': term_score\n",
        "          }\n",
        "      explanation['total_score'] = total_score\n",
        "      return explanation\n",
        "  def get_statistics(self) -> Dict:\n",
        "      \"\"\"Статистика индекса\"\"\"\n",
        "      return {\n",
        "      'num_documents': len(self.documents),\n",
        "      'vocabulary_size': len(self.vocabulary),\n",
        "      'average_document_length': self.average_document_length,\n",
        "      'total_terms': sum(self.document_lengths),\n",
        "      'parameters': {'k1': self.k1, 'b': self.b}\n",
        "      }\n",
        "\n"
      ],
      "metadata": {
        "id": "jzQgb6qEBn8C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оптимизированная реализация для больших корпусов"
      ],
      "metadata": {
        "id": "sGWl1Pxg211N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "class OptimizedBM25:\n",
        "  \"\"\"Оптимизированная версия BM25 для больших корпусов\"\"\"\n",
        "  def __init__(self, k1=1.2, b=0.75):\n",
        "      self.k1 = k1\n",
        "      self.b = b\n",
        "      self.vocabulary_ = {}\n",
        "      self.idf_ = None\n",
        "      self.doc_len = None\n",
        "      self.avgdl = 0.0\n",
        "      self.doc_freqs = None\n",
        "      self.tf_matrix = None\n",
        "  def fit(self, corpus):\n",
        "      \"\"\"Быстрое построение индекса с scipy.sparse\"\"\"\n",
        "      # Токенизация и построение словаря\n",
        "      tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
        "      vocabulary = set()\n",
        "      for doc in tokenized_corpus:\n",
        "        vocabulary.update(doc)\n",
        "      self.vocabulary_ = {term: i for i, term in enumerate(vocabulary)}\n",
        "      # Построение разреженной TF-матрицы\n",
        "      self._build_tf_matrix(tokenized_corpus)\n",
        "      # Вычисление статистик\n",
        "      self.doc_len = np.array([len(doc) for doc in tokenized_corpus])\n",
        "      self.avgdl = self.doc_len.mean()\n",
        "      # Вычисление IDF\n",
        "      self._calculate_idf()\n",
        "      return self\n",
        "  def _build_tf_matrix(self, tokenized_corpus):\n",
        "      \"\"\"Построение разреженной матрицы частот терминов\"\"\"\n",
        "      rows, cols, data = [], [], []\n",
        "      for doc_idx, doc in enumerate(tokenized_corpus):\n",
        "        term_counts = Counter(doc)\n",
        "        for term, count in term_counts.items():\n",
        "          if term in self.vocabulary_:\n",
        "            rows.append(doc_idx)\n",
        "            cols.append(self.vocabulary_[term])\n",
        "            data.append(count)\n",
        "            self.tf_matrix = sparse.csr_matrix(\n",
        "            (data, (rows, cols)),\n",
        "            shape=(len(tokenized_corpus), len(self.vocabulary_))\n",
        "            )\n",
        "  def _calculate_idf(self):\n",
        "      \"\"\"Векторизованное вычисление IDF\"\"\"\n",
        "      N = self.tf_matrix.shape[0]\n",
        "      # Количество документов, содержащих каждый термин\n",
        "      df = np.array((self.tf_matrix > 0).sum(axis=0)).flatten()\n",
        "      # BM25 IDF формула\n",
        "      self.idf_ = np.log((N - df + 0.5) / (df + 0.5))\n",
        "      # Обнуление отрицательных значений\n",
        "      self.idf_ = np.maximum(self.idf_, 0)\n",
        "  def transform(self, query):\n",
        "      \"\"\"Быстрый поиск с векторизованными операциями\"\"\"\n",
        "      if isinstance(query, str):\n",
        "        query_terms = query.lower().split()\n",
        "      else:\n",
        "        query_terms = query\n",
        "      # Построение вектора запроса\n",
        "      query_vec = np.zeros(len(self.vocabulary_))\n",
        "      for term in query_terms:\n",
        "        if term in self.vocabulary_:\n",
        "          query_vec[self.vocabulary_[term]] += 1\n",
        "      # Векторизованное вычисление BM25\n",
        "      tf_matrix_dense = self.tf_matrix.toarray()\n",
        "      # Нормализация длины документов\n",
        "      doc_lens = self.doc_len.reshape(-1, 1)\n",
        "      length_norm = 1 - self.b + self.b * (doc_lens / self.avgdl)\n",
        "      # BM25-формула в векторизованном виде\n",
        "      numerator = tf_matrix_dense * (self.k1 + 1)\n",
        "      denominator = tf_matrix_dense + self.k1 * length_norm\n",
        "      tf_component = numerator / denominator\n",
        "      # Применение IDF и запроса\n",
        "      scores = np.sum(tf_component * self.idf_ * query_vec, axis=1)\n",
        "      return scores\n",
        "  # Пример использования оптимизированной версии\n",
        "def demo_optimized_bm25():\n",
        "      \"\"\"Демонстрация оптимизированного BM25\"\"\"\n",
        "      documents = [\n",
        "      \"Python – мощный язык программирования для анализа данных\",\n",
        "      \"Машинное обучение использует алгоритмы для анализа больших данных\",\n",
        "      \"RAG-системы объединяют поиск информации с генерацией ответов\",\n",
        "      \"Векторные базы данных оптимизированы для семантического поиска\",\n",
        "      \"BM25-алгоритм превосходит TF-IDF в задачах информационного поиска\"\n",
        "      ]\n",
        "      # Инициализация и обучение\n",
        "      # bm25 = OptimizedBM25(k1=1.5, b=0.75)\n",
        "      bm25 = OptimizedBM25(k1=1.5, b=0.75)\n",
        "      bm25.fit(documents)\n",
        "      # Поиск\n",
        "      query = \"анализ данных Python\"\n",
        "      scores = bm25.transform(query)\n",
        "      # Вывод результатов\n",
        "      ranked_docs = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
        "      print(f\"Результаты поиска для запроса: '{query}'\")\n",
        "      print(\"-\" * 50)\n",
        "      for doc_idx, score in ranked_docs:\n",
        "        if score > 0:\n",
        "            print(f\"Документ {doc_idx} (скорость: {score:.4f}):\")\n",
        "\n",
        "            print(f\" {documents[doc_idx]}\")\n",
        "            print()\n",
        "      return bm25, documents\n",
        "if __name__ == \"__main__\":\n",
        "    demo_optimized_bm25()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWFK1sgK22Bc",
        "outputId": "5f6ac83c-ea81-4ae0-d0b1-72ad63b53526"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результаты поиска для запроса: 'анализ данных Python'\n",
            "--------------------------------------------------\n",
            "Документ 0 (скорость: 1.0599):\n",
            " Python – мощный язык программирования для анализа данных\n",
            "\n"
          ]
        }
      ]
    }
  ]
}